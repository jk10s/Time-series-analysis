# -*- coding: utf-8 -*-
"""Time_series_temperature_Shan_singh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1azSeEY1yp67VIM6D-I6LmLPWoNl2F2PI
"""



## Importing necessary libraries..

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



"""# 1.. Lets Pre-process our data
    Let's read the data from the "GlobalTemperatures.csv" file, 
    which has a monthly Earth’s temperature and plot it on the 
    chart.
"""

global_temp=pd.read_csv(r'F:\Time_series_Analysis\2. Global_Warming\Dataset/GlobalTemperatures.csv')

global_temp.head(3)

"""### how to clean/prepare/pre-process data :
    a.. remove duplicate rows
    b.. remove irrelevant data.
    c.. Fix structural errors.
    d.. check missing values in data
    e.. check data-type of each feature is right or not..
    f.. deal with the outliers
"""

global_temp.duplicated().sum()

### ie no duplicated observations we have !

global_temp.dtypes

### date-type of 'dt' must be 'Date-time' bcz it holds date-time values..

'''
in date-time , we have 2 data-type (datetime64[ns]) and (<M8[ns])
datetime64[ns] is a general dtype, while <M8[ns] is a specific dtype ,
ns is basicaly nano second..
Both are similar , it entirely how your numpy was compiled..

'''

global_temp['dt'][0]

global_temp['dt']=pd.to_datetime(global_temp['dt'])

global_temp['dt'].dtype

global_temp['dt'][0]



"""# 2.. Does Global Warming exists ?"""

## extracting year from the 'dt' feature..
global_temp['years']=global_temp['dt'].dt.year

global_temp.head(2)

## finding "avg_LandAverageTemperature" & "avg_LandAverageTemperatureUncertainty" for each of the year..

data=global_temp.groupby('years').agg({'LandAverageTemperature':'mean','LandAverageTemperatureUncertainty':'mean'}).reset_index()

data

data.columns

"""    Maximum Uncertainty will be Avg temperature  +   deviation(Uncertainty)

    whereas minimum Uncertainty will be Avg temp -   deviation(Uncertainty)

    and this deviation is exactly your Uncertainty...
"""

## so lets create 2 more features as 'Uncertainty_top' & 'Uncertainty_bottom'

data['Uncertainty_top']=data['LandAverageTemperature'] + data['LandAverageTemperatureUncertainty']
data['Uncertainty_bottom']=data['LandAverageTemperature'] - data['LandAverageTemperatureUncertainty']

data.head(2)

data.columns

### pandas lineplot :

data.plot(x='years',y=['LandAverageTemperature', 'LandAverageTemperatureUncertainty',
       'Uncertainty_top', 'Uncertainty_bottom'],figsize=(16,10))



"""    Conclusion ::  From the charts you can see, that there is global warming nowadays. 
        The average temperature of Earth surface has the highest value in 
        the last three centuries. The fastest temperature growth occurred 
        in the last 30 years! This worries me, I hope soon humanity will 
        fully switch to ecological sources of energy, that will reduce CO2. 
        If it’s will not happened, we will be in disaster. This charts also 
        have confidence intervals, which shows that measurement of temperatur
        e has become more accurate in the last few years.
"""







"""# 3.. Analyse Average temperature in each season ?"""

global_temp.columns

## extract 'month' from 'dt' feature.
global_temp['month']=global_temp['dt'].dt.month

global_temp.head(2)

global_temp['month'].unique()



## lets define a function to extract season from 'month'

def get_season(month):
    
    if month>=3 and month<=5:
        return 'spring'
    
    elif month>=6 and month<=8:
        return 'summer'
    
    elif month>=9 and month<=11:
        return 'autumn'
    
    else:
        return 'winter'

## applying a function on top of 'month' feature..

global_temp['season']=global_temp['month'].apply(get_season)

global_temp.columns

years=global_temp['years'].unique()



global_temp['season'].unique()





## lets define 4 blank lists to store all the "season temp"  of "various years".

spring_temps=[]
summer_temps=[]
autumn_temps=[]
winter_temps=[]

for year in years:
    current_yr=global_temp[global_temp['years']==year]
    
    spring_temps.append(current_yr[current_yr['season']=='spring']['LandAverageTemperature'].mean())
    summer_temps.append(current_yr[current_yr['season']=='summer']['LandAverageTemperature'].mean())
    autumn_temps.append(current_yr[current_yr['season']=='autumn']['LandAverageTemperature'].mean())
    winter_temps.append(current_yr[current_yr['season']=='winter']['LandAverageTemperature'].mean())



### lets make a blank dataframe ..

season=pd.DataFrame()

## assigning features in dataframe..

season['years']=years
season['spring_temps']=spring_temps
season['summer_temps']=summer_temps
season['autumn_temps']=autumn_temps
season['winter_temps']=winter_temps

season

##data.plot(x='years',y=['LandAverageTemperature', 'LandAverageTemperatureUncertainty',
 ##      'Uncertainty_top', 'Uncertainty_bottom'],figsize=(16,10))

season.columns

season[['spring_temps', 'summer_temps', 'autumn_temps',
       'winter_temps']]

plt.figure(figsize=(16,10))
plt.plot(season['years'],season[['spring_temps', 'summer_temps', 'autumn_temps',
       'winter_temps']])

plt.legend(['spring_temps', 'summer_temps', 'autumn_temps',
       'winter_temps'])

"""    Is it getting warmer? Yes, it is."""





"""# 4.. how to prepare your data for Time series Modelling..
    Prepare your data for Modelling.. (lets do sampling 
        of data & then pre-processing of data )
"""

### lets read 'GlobalLandTemperaturesByCity.csv'

cities=pd.read_csv(r'F:\Time_series_Analysis\2. Global_Warming\Dataset/GlobalLandTemperaturesByCity.csv')

cities.shape

"""#### how to do sampling in data..
    1.. Random sampling
    2.. we say pick first 1M or last 1M points..
    3.. we say we pick data for Top economies.. (Russia ,USA ,
             UK ,India ,France etc.. )
             
         or pick data for some cities of USA , eg 
         ['New York','Los Angeles','San Francisco' ,'Mumbai' ,'Delhi']
"""

## finding unique countries in 'country' feature..

cities['Country'].unique()

## creating separate datafrme for 'United States' only..

usa=cities[cities['Country']=='United States']

usa.columns

## unique cities in 'usa'..

usa['City'].unique()

## considering data of ['New York','Los Angeles','San Francisco'] in my usa dataframe..

usa_cities=['New York','Los Angeles','San Francisco']

data2=usa[usa['City'].isin(usa_cities)]

data2.shape

data2.head(3)

data2=data2[['dt','AverageTemperature']]

data2.head(2)

## assigning your own column name..

data2.columns=['Date','Temp']

data2

data2.dtypes

## converting data-type of 'Date' feature into date-time ..

data2['Date']=pd.to_datetime(data2['Date'])

data2.dtypes

## total missing values in each of the feature..

data2.isnull().sum()

## dropping the missing values..

data2.dropna(inplace=True)

data2.head(3)

"""    note:   for Machine Learning,feature is a columns & we have 
               by-default index as from 0 but for our time series 
               Problem,we have date column our as row-index
"""

data2.set_index('Date',inplace=True)

data2.head(4)







"""# 5.. How to find whether data has Seasonality factor or not..
      we will try to convert our seasonal data into stationary bcz Time SEries algos works phenomenal with stationary data..

    lets understand what is seasoanlity ,stationary & couple of Time series Terms..

    seasonality ->> When mean & variance is not constant throughout the data ,
                    ie data has a trend here..
                    
    stationary data ->>  so data that has no trend ie that has constant 
                       mean & std dev throughout data..
                       
    if not stationary then we have to make it stationary using various manipulations..


    Seasonality is opposite of stationary ,ie 
        A time series with a clear seasonal component is referred to as non-stationary...
        
        ie the goal is we have to convert seasonal data into stationary data..
        
        
        
        a)Visualisation approach
    
    (seasonality is all about suppose in each yr in christmas sales goes up) and then down ,
       ie this is a cycle that repeats over time, such as monthly or yearly. 
       
       There are many types of seasonality; 
       for example:

    Time of Day.
    Daily.
    Weekly.
    Monthly.
    Yearly.
    Once seasonality is identified, it can be removed

    The model of seasonality can be removed from the time series. 
    This process is called Seasonal Adjustment, or Deseasonalizing.
    A time series where the seasonal component has been removed is called seasonal stationary.
    A time series with a clear seasonal component is referred to as non-stationary.
https://machinelearningmastery.com/time-series-seasonality-with-python/

        
"""



###  i'm going to create a pivot table to plot the monthly temperatures through the years
## so I need 'year' & 'month' feature..

data2['year']=data2.index.year

data2['month']=data2.index.month

data2.head(3)



## creating a pivot table :

pivot=data2.pivot_table(values='Temp',index='month',columns='year')

pivot

## if we want to plot with respect to time or index of dataframe ,  we can use pivot.plot() function of pandas ..


pivot.plot(figsize=(20,8))
plt.legend().remove()
plt.xlabel('Months')
plt.ylabel('Temperatures')

"""    looking this graph,we can say this data is seasonal
        The series clearly has some seasonality, the higher temperatures 
        are around b/w June & August and the lower are between December & Feb
        so thats basically a observation that u can drawn and represent to ur client..

"""





"""# 6.. How to find whether data is stationary or not..

### ways to find that...
    a.. using data visualisation (lineplot to find out 
                               whats a trend of data)
                               
    b.. Evaluating the descriptive statistics ( ie compute
                 mean & variance of various chunks of data )
https://www.geeksforgeeks.org/how-to-check-if-time-series-data-is-stationary-with-python/

    c.. using statistical Tests (AD-Fuller , KPSS Test )
"""

### checking whether data is stationary or not..

data2['Temp'].plot(figsize=(20,12))

'''
It seems that data is seasonal in nature.. The spread of the data indicates that there is a significant variation in the data. 
To flatten the growing variance, we need to transform the data.

A recurring pattern with a defined and predictable regularity dependent on the time of year, week, or day 
is referred to as seasonality.



'''



"""#### b.. using statistical Tests
https://analyticsindiamag.com/how-to-make-a-time-series-stationary/


    >> Ad-fuller Test
    >> Kwiatkowski Phillips Schmidt Shin (KPSS) test:
"""

### Testing For Stationarity using Augmented Dickey-Fuller test..

from statsmodels.tsa.stattools import adfuller

adfuller(data2['Temp'])

'''
adfuller gives 5 values ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used','critical values']

most imp is p-value
p-value concept is almost similar to Hypothesis testing,
Hypothesis Testing have 2 Hypothesis(Alternate & Null Hypothesis)

Null hypo says Data is not Stationery
whereas Alternate hypo says data is stationery

lags are those which are our previous Data

'''

#if Pvalue<0.05 ,we reject Null hypo ie alternate hypo is 
# true,ie data is stationery

#Ho: ie null hypoIt is non stationary
#H1: ie Alternate Hypo ,It is stationary
#zip basically combines result,labels

def adfuller_test(data):
    result=adfuller(data)
    
    labels=['ADF Test statistics','p-value','lags_used','nobs']
    
    for value,label in zip(result,labels):
        print(label + ' : ' + str(value))
    if result[1] <=0.05:
        print('reject the Ho ,data is stationary')
    else:
        print('fail to reject the Ho ,data is not stationary')

adfuller_test(data2['Temp'])



"""# 7.. How to make your data stationary ..

#### Now data is not non-stationary ,ie we have to make it stationary..
    using log , sq root , cube root , differencing etc...
"""



##data2['Temp'].head(10)

##data2['Temp'].shift(3).head(10)

data2['first_temp_diff']=data2['Temp'] - data2['Temp'].shift(12)

'''
why taken shift(12) , bcz basically year has 12 month cycle , ie in order to remove seasonal component..

In previous diagram of pivot table , we have figured out that we have yearly seasonality.. , 
so lets use shift(12) to remove seasonality 

'''

data2.head(13)

## Again perform Aug.dickey fuller test on df['first_diff_temp']

adfuller_test(data2['first_temp_diff'].dropna())

''
now our p-value is less than 0.05 which basically says we are rejecting null hypo and accepting alternate hypo
ie data is stationery

Note : if p-value is almost 0, then we can say , we have a wonderful stationary graph

'''



data2['first_temp_diff'].plot(figsize=(20,6))







"""# 8.. Lets build Time Series model..
    Lets Build a basic moving Avg model

types of models in Time-Series :
         1.Base line model
         2.Exponential model
         3.AR model
         4.MA model
         5.ARIMA model
         6.SARIMA model(Seasonal ARIMA)
"""

df=data2[['first_temp_diff']]

df.dropna(inplace=True)

import warnings
from warnings import filterwarnings
filterwarnings('ignore')

df.head(10)

### Smoothening your series using moving average
## Rolling is just like a window that you are going to consider or I can say its a window that we have considered for Moving average,
## lets say 5 is the window size or 5 is the business period


df['Predictions']=df['first_temp_diff'].rolling(window=5).mean()

df.head(2)

df.dropna(inplace=True)

df.head(3)

df.columns=['actual_temp','forecast_temp']

df.head(3)



### check accuracy of your model using RMSE..

from sklearn.metrics import mean_squared_error

np.sqrt(mean_squared_error(df['actual_temp'],df['forecast_temp']))

## why np.sqrt, bcz mean_squared_error gives square of error and then by calling np.sqrt we get actual error

"""we are getting error of 2.4 degree celsius every day, it means error of +-2.39 is going to happen in your prediction."""





"""# 9.. Lets Build ARIMA Model.."""

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
from statsmodels.graphics.api import qqplot

df.head(3)

plot_acf(df['actual_temp'])
plt.show()
## q-3

plot_pacf(df['actual_temp'])
plt.show()
## p-2

df.head(3)

df=df[['actual_temp']]

df.head(2)

df.shape

training_data=df[0:6000]
testing_data=df[6000:]

### create test data that is basically unseen to our model or which is basically is used for evalaution



from statsmodels.tsa.arima_model import ARIMA

### arima= ARIMA(training_data,order=(p,d,q))

arima=ARIMA(training_data,order=(2,1,3))

model=arima.fit()

## doing predictions..

pred=model.forecast(steps=len(testing_data))[0]

pred

len(pred)

len(testing_data)

np.sqrt(mean_squared_error(testing_data,pred))

'''
now u will see it is better than MA model that we have created having Error as (2.4 degree centigrade) 
has decreased to 1.5 which shows that arima model is much more good in such sceanario rather than Baseline 

'''









"""# 10.. Lets do model Tuning or Hyperparameter Tuning.."""

#### now define hyper-para=meters 

p_values=range(0,4)
d_values=range(0,3)
q_values=range(0,4)

### so we are just trying to provide pairs of (p,d,q) & whichever pair will give me least error i will consider that pair, 


for p in p_values:
    for d in d_values:
        for q in q_values:
            order=(p,d,q)
            
            train=df[0:6000]
            test=df[6000:]
            
            predictions=[]
            
            ### now we have train & test data that we can pass to our algorithm
            for i in range(len(test)):
                try:
                    arima=ARIMA(train,order)
                    
                    ## Role of disp(displacement) is to control the frequency of the output during the iterations. 
                    
                    model=arima.fit(disp=0)
                    pred_y=model.forecast()[0]
                    predictions.append(pred_y)
                    error=mean_squared_error(test,predictions)
                    print('MSE is {} with order {}'.format(error,order))

                except:
                    continue









